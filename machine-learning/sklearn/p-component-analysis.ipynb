{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95077309, -0.2308092 ],\n",
       "       [-0.73140197,  0.17978878],\n",
       "       [-0.86075863,  0.12277505],\n",
       "       [-0.54385048,  0.46489064],\n",
       "       [-0.75918085,  0.08235719],\n",
       "       [-0.8407742 , -0.14914684],\n",
       "       [-0.80614298,  0.05262986],\n",
       "       [-0.5699995 ,  0.46299731],\n",
       "       [-0.63838938,  0.46043219],\n",
       "       [-0.12680859, -0.37222046],\n",
       "       [-0.81854718,  0.16379344],\n",
       "       [-0.56011143, -0.33661634],\n",
       "       [ 0.04896666, -0.37941349],\n",
       "       [ 0.99067679, -0.26037304],\n",
       "       [-0.40677968,  0.55978873],\n",
       "       [ 1.45424458,  0.19837986],\n",
       "       [ 0.75250957, -0.30023799],\n",
       "       [-0.57755635,  0.61374639],\n",
       "       [ 1.46715923,  0.38406555],\n",
       "       [ 1.53831002,  0.50261502],\n",
       "       [-0.48078354,  0.62266439],\n",
       "       [-0.83036756, -0.15308684],\n",
       "       [ 1.50122318,  0.18332701],\n",
       "       [ 1.20815351,  0.00970269],\n",
       "       [-0.78449853,  0.10474145],\n",
       "       [-0.86133263, -0.0158546 ],\n",
       "       [-0.52899914, -0.34031688],\n",
       "       [ 0.03173058, -0.44387507],\n",
       "       [ 0.78758945, -0.2175903 ],\n",
       "       [-0.56834769, -0.28382515],\n",
       "       [-0.18865895, -0.45692564],\n",
       "       [ 1.2942033 ,  0.04421527],\n",
       "       [-0.46362271,  0.53605098],\n",
       "       [-0.75372279, -0.0811211 ],\n",
       "       [ 1.12433712, -0.09247809],\n",
       "       [ 1.58075419,  0.47969819],\n",
       "       [ 0.07383098, -0.5419888 ],\n",
       "       [ 0.19711835, -0.38342463],\n",
       "       [-0.61680983,  0.38407736],\n",
       "       [-0.21748882, -0.36696135],\n",
       "       [ 0.14975376, -0.38321717],\n",
       "       [-0.80431944, -0.05772424],\n",
       "       [-0.85034526, -0.13052436],\n",
       "       [-0.47275223, -0.28917448],\n",
       "       [-0.36706684,  0.58482448],\n",
       "       [-0.7900553 , -0.28616175],\n",
       "       [-0.7049589 ,  0.0323904 ],\n",
       "       [-0.67817551, -0.35645409],\n",
       "       [-0.71507795,  0.38791686],\n",
       "       [ 1.5100324 ,  0.4890142 ],\n",
       "       [ 0.56637025, -0.17876735],\n",
       "       [ 0.9740839 , -0.27270233],\n",
       "       [ 1.35009923,  0.0867067 ],\n",
       "       [ 0.04935162, -0.37028965],\n",
       "       [-0.65468246,  0.44029464],\n",
       "       [ 0.22531899, -0.33588242],\n",
       "       [ 0.47782183, -0.43754903],\n",
       "       [ 1.27355254,  0.02774897],\n",
       "       [-0.28135498, -0.38619353],\n",
       "       [-0.72424282,  0.22927259]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "# Running PCS on sklearn is very easy\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "# 2D projection\n",
    "X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can recover the 3D projection\n",
    "X3D_inv = pca.inverse_transform(X2D)\n",
    "\n",
    "# There will be some loss of information\n",
    "np.allclose(X3D_inv, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011314282069198146"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can calculate the error of returning to a 3D representation\n",
    "np.mean(np.sum(np.square(X3D_inv - X), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.98196733, -0.13489777, -0.13244907],\n",
       "       [ 0.16476254, -0.95420546, -0.2496903 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the principal components\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84212839, 0.14390061])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the explanation capabilities of each dimension\n",
    "pca.explained_variance_ratio_\n",
    "# First dimension explains 84.2% of the variance, second explains 14.6%\n",
    "# Note the third dimension explains 100%-(84%+14%) = 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01397100070575763"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at variance lost due to 2D projection\n",
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "\n",
    "Usually we want to reduce the number of dimensions up to a certain variance (unless we want to visualize the data, in which we are stuck to either 2 or 3 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Summing variances\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# Now we woulf only need to re-run with n_components=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An alternative is to directly specify the variance threshold\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
